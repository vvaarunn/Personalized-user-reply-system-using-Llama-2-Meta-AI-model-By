# -*- coding: utf-8 -*-
"""Test1_Big_Bear_Diner_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pYS_mNvn5cn8gH7TQsneJmP5s0s37PbZ

#**Step 1: Installing all Packages**
"""

!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7

"""#**Step 2: Importing all Libraries**"""

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig, PeftModel
from trl import SFTTrainer

"""#**Step 3**"""

# The model that you want to train from the Hugging Face hub
model_name = "NousResearch/Llama-2-7b-chat-hf"

# The instruction dataset to use
#dataset_name = "mlabonne/guanaco-llama2-1k"
dataset_name = "formated_data_4.txt"

# Fine-tuned model name
new_model = "Llama-2-7b-chat-finetune_bigbeardiner"

# Read the contents of the .txt file
with open("formatted_data.txt", "r") as file:
    data = file.readlines()

# Organize the data into a suitable data structure
formatted_data = []
for line in data:
    formatted_data.append({"text": line.strip()})

# Convert the data into a format compatible with load_dataset
dataset_dict = {
    "text": [example["text"] for example in formatted_data]
}

# Create a Dataset object
from datasets import Dataset
dataset = Dataset.from_dict(dataset_dict)

# Print information about the dataset
print(dataset)

"""#**Step 4**"""

# Load tokenizer and model with QLoRA configuration
compute_dtype = getattr(torch, "float16")

bnb_config = BitsAndBytesConfig(
    load_in_4bit= True,
    bnb_4bit_quant_type= "nf4",
    bnb_4bit_compute_dtype=  "float16",
    bnb_4bit_use_double_quant= False,
)

# Check GPU compatibility with bfloat16
if compute_dtype == torch.float16 and True:
    major, _ = torch.cuda.get_device_capability()
    if major >= 8:
        print("=" * 80)
        print("Your GPU supports bfloat16: accelerate training with bf16=True")
        print("=" * 80)

# Load base model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config= bnb_config,
    device_map= {"": 0}
)
model.config.use_cache = False
model.config.pretraining_tp = 1

# Load LLaMA tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right" # Fix weird overflow issue with fp16 training

# Load LoRA configuration
peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r = 64,
    bias="none",
    task_type="CAUSAL_LM",
)

# Set training parameters
training_arguments = TrainingArguments(
    output_dir= "./results",
    num_train_epochs= 2,
    per_device_train_batch_size = 4,
    gradient_accumulation_steps = 1,
    optim = "paged_adamw_32bit",
    save_steps = 0,
    logging_steps = 25,
    learning_rate = 2e-4,
    weight_decay = 0.001,
    fp16 = False,
    bf16 = False,
    max_grad_norm = 0.3,
    max_steps = -1,
    warmup_ratio = 0.03,
    group_by_length = True,
    lr_scheduler_type = "cosine",
    report_to="tensorboard"
)

# Set supervised fine-tuning parameters
trainer = SFTTrainer(
   model = model,
   train_dataset = dataset,
   peft_config = peft_config,
   dataset_text_field="text",
   max_seq_length = None,
   tokenizer = tokenizer,
   args = training_arguments,
   packing = False,
)

# Train model
trainer.train()

# Save trained model
trainer.model.save_pretrained(new_model)

"""###**Step 5**"""

# Ignore warnings
#logging.set_verbosity(logging.CRITICAL)

# Run text generation pipeline with our model
prompt = input("Enter your comment to generate an automated reply: ")
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=60)
result = pipe(f"<s>[INST] {prompt} [/INST]")

# Get generated text and remove the prompt part
generated_text = result[0]['generated_text']
generated_text = generated_text.split("[/INST]")[1].strip()

print(generated_text)

